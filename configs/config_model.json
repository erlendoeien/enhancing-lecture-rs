{
    "attn_type": "bi",
    "hidden_act": "gelu",
    "summary_type": "last",
    "total_seq_length": 30,
    "d_model": {
        "min": 64,
        "max": 512,
        "step": 64,
        "type_": "int"
    },
    "n_layer": {
        "min": 1,
        "max": 8,
        "step": 1,
        "type_": "int"
    },
    "n_head": [
        1,
        2,
        4,
        8,
        16
    ],
    "dropout": {
        "min": 0.0,
        "max": 0.5,
        "step": 0.1,
        "type_": "float"
    },
    "layer_norm_eps": {
        "min": 1e-08,
        "max": 1e-3,
        "log": true,
        "type_": "float"
    }
}